# ðŸ€ Ratatouille v2 - Architecture Rework

> **Status**: APPROVED
> **Date**: 2026-02-04
> **Decisions**: Final

---

## ðŸŽ¯ Final Architecture Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| **Table Format** | Iceberg + Nessie | Git-like branching, multi-tenant, REST API for permissions |
| **Query Engine** | DuckDB only | Embedded, low memory, no extra service, perfect for 20GB VM |
| **SDK Style** | dbt-like native | SQL + YAML + Python escape hatch - scales to 10B rows |
| **Data Sharing** | Data Products | Publish/Subscribe with versioning and permissions |
| **Partitioning** | Configurable per pipeline | Each pipeline defines its own strategy |
| **Resources** | Balanced profiles | Configurable for tiny â†’ large VMs |

---

## ðŸ—ï¸ Target Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RATATOUILLE v2                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DATA PRODUCTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     (Shared Catalog - Versioned)                       â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚ â”‚
â”‚  â”‚  â”‚ sales_kpis â”‚  â”‚ inventory  â”‚  â”‚ customers  â”‚                       â”‚ â”‚
â”‚  â”‚  â”‚ v2.1.0     â”‚  â”‚ v1.0.0     â”‚  â”‚ v3.0.0     â”‚      publish â†‘       â”‚ â”‚
â”‚  â”‚  â”‚ owner:ws-a â”‚  â”‚ owner:ws-b â”‚  â”‚ owner:ws-a â”‚      subscribe â†“     â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    â”‚                                        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚          â–¼                         â–¼                         â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚    WORKSPACE A    â”‚  â”‚    WORKSPACE B    â”‚  â”‚    WORKSPACE C    â”‚      â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚      â”‚
â”‚  â”‚  pipelines/       â”‚  â”‚  pipelines/       â”‚  â”‚  pipelines/       â”‚      â”‚
â”‚  â”‚   â”œâ”€ bronze/      â”‚  â”‚   â”œâ”€ bronze/      â”‚  â”‚   â”œâ”€ bronze/      â”‚      â”‚
â”‚  â”‚   â”‚  â””â”€ *.py      â”‚  â”‚   â”‚  â””â”€ *.py      â”‚  â”‚   â”‚  â””â”€ *.py      â”‚      â”‚
â”‚  â”‚   â”œâ”€ silver/      â”‚  â”‚   â”œâ”€ silver/      â”‚  â”‚   â”œâ”€ silver/      â”‚      â”‚
â”‚  â”‚   â”‚  â”œâ”€ *.sql     â”‚  â”‚   â”‚  â”œâ”€ *.sql     â”‚  â”‚   â”‚  â”œâ”€ *.sql     â”‚      â”‚
â”‚  â”‚   â”‚  â””â”€ *.yaml    â”‚  â”‚   â”‚  â””â”€ *.yaml    â”‚  â”‚   â”‚  â””â”€ *.yaml    â”‚      â”‚
â”‚  â”‚   â””â”€ gold/        â”‚  â”‚   â””â”€ gold/        â”‚  â”‚   â””â”€ gold/        â”‚      â”‚
â”‚  â”‚      â”œâ”€ *.sql     â”‚  â”‚      â”œâ”€ *.sql     â”‚  â”‚      â”œâ”€ *.sql     â”‚      â”‚
â”‚  â”‚      â””â”€ *.yaml    â”‚  â”‚      â””â”€ *.yaml    â”‚  â”‚      â””â”€ *.yaml    â”‚      â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚      â”‚
â”‚  â”‚  Isolated:        â”‚  â”‚  Isolated:        â”‚  â”‚  Isolated:        â”‚      â”‚
â”‚  â”‚  â€¢ Nessie branch  â”‚  â”‚  â€¢ Nessie branch  â”‚  â”‚  â€¢ Nessie branch  â”‚      â”‚
â”‚  â”‚  â€¢ S3: ws-a/*     â”‚  â”‚  â€¢ S3: ws-b/*     â”‚  â”‚  â€¢ S3: ws-c/*     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CORE SERVICES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚   â”‚   Nessie    â”‚    â”‚   DuckDB    â”‚    â”‚   MinIO     â”‚              â”‚  â”‚
â”‚  â”‚   â”‚  (Catalog)  â”‚    â”‚  (Engine)   â”‚    â”‚    (S3)     â”‚              â”‚  â”‚
â”‚  â”‚   â”‚             â”‚    â”‚             â”‚    â”‚             â”‚              â”‚  â”‚
â”‚  â”‚   â”‚ â€¢ Branches  â”‚    â”‚ â€¢ Embedded  â”‚    â”‚ â€¢ Parquet   â”‚              â”‚  â”‚
â”‚  â”‚   â”‚ â€¢ Commits   â”‚    â”‚ â€¢ Low RAM   â”‚    â”‚ â€¢ Iceberg   â”‚              â”‚  â”‚
â”‚  â”‚   â”‚ â€¢ REST API  â”‚    â”‚ â€¢ 10B rows  â”‚    â”‚ â€¢ Products  â”‚              â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚  â”‚   â”‚   Dagster   â”‚    â”‚  Product    â”‚    â”‚  Lineage    â”‚              â”‚  â”‚
â”‚  â”‚   â”‚  (Orchestr) â”‚    â”‚  Registry   â”‚    â”‚  Tracker    â”‚              â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚  â”‚                                                                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ New Project Structure

```
ratatouille/
â”‚
â”œâ”€â”€ src/ratatouille/
â”‚   â”œâ”€â”€ __init__.py              # Exports: rat, pipeline, ref, etc.
â”‚   â”œâ”€â”€ rat.py                   # Main SDK entry point (simplified)
â”‚   â”‚
â”‚   â”œâ”€â”€ workspace/               # ðŸ†• Workspace Management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py           # Workspace CRUD, isolation
â”‚   â”‚   â”œâ”€â”€ config.py            # WorkspaceConfig (Pydantic)
â”‚   â”‚   â””â”€â”€ context.py           # Current workspace context
â”‚   â”‚
â”‚   â”œâ”€â”€ catalog/                 # ðŸ†• Nessie Catalog Integration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ nessie.py            # Nessie REST client
â”‚   â”‚   â”œâ”€â”€ iceberg.py           # Iceberg table operations
â”‚   â”‚   â””â”€â”€ branches.py          # Branch management
â”‚   â”‚
â”‚   â”œâ”€â”€ engine/                  # ðŸ†• DuckDB Query Engine
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ duckdb.py            # DuckDB connection + queries
â”‚   â”‚   â”œâ”€â”€ iceberg_ext.py       # DuckDB Iceberg extension
â”‚   â”‚   â””â”€â”€ memory.py            # Memory management
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/                # ðŸ†• dbt-like Pipeline Framework
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py            # Load SQL + YAML files
â”‚   â”‚   â”œâ”€â”€ parser.py            # Parse {{ ref() }}, {% if %}
â”‚   â”‚   â”œâ”€â”€ compiler.py          # Compile to executable SQL
â”‚   â”‚   â”œâ”€â”€ executor.py          # Run pipelines
â”‚   â”‚   â”œâ”€â”€ incremental.py       # Incremental logic
â”‚   â”‚   â””â”€â”€ python_bridge.py     # Python pipeline support
â”‚   â”‚
â”‚   â”œâ”€â”€ products/                # ðŸ†• Data Products
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ registry.py          # Product catalog (SQLite)
â”‚   â”‚   â”œâ”€â”€ publish.py           # Publish tables as products
â”‚   â”‚   â”œâ”€â”€ consume.py           # Consume products
â”‚   â”‚   â””â”€â”€ permissions.py       # Access control
â”‚   â”‚
â”‚   â”œâ”€â”€ schema/                  # ðŸ†• Schema Management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ validator.py         # Validate DataFrames
â”‚   â”‚   â”œâ”€â”€ tests.py             # not_null, unique, positive, etc.
â”‚   â”‚   â””â”€â”€ evolution.py         # Schema evolution
â”‚   â”‚
â”‚   â”œâ”€â”€ lineage/                 # ðŸ†• Lineage Tracking
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tracker.py           # Record transformations
â”‚   â”‚   â””â”€â”€ graph.py             # Query lineage DAG
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                 # S3/Parquet Operations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ s3.py                # MinIO/S3 client
â”‚   â”‚   â””â”€â”€ parquet.py           # Parquet read/write
â”‚   â”‚
â”‚   â””â”€â”€ resources/               # Resource Management
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py            # ResourceConfig
â”‚       â””â”€â”€ profiles.py          # tiny/small/medium/large
â”‚
â”œâ”€â”€ workspaces/                  # Workspace Instances
â”‚   â””â”€â”€ default/
â”‚       â”œâ”€â”€ workspace.yaml       # Workspace configuration
â”‚       â”œâ”€â”€ pipelines/
â”‚       â”‚   â”œâ”€â”€ bronze/          # Python ingestion
â”‚       â”‚   â”‚   â””â”€â”€ ingest_*.py
â”‚       â”‚   â”œâ”€â”€ silver/          # SQL transforms
â”‚       â”‚   â”‚   â”œâ”€â”€ *.sql
â”‚       â”‚   â”‚   â””â”€â”€ *.yaml
â”‚       â”‚   â””â”€â”€ gold/            # SQL aggregations
â”‚       â”‚       â”œâ”€â”€ *.sql
â”‚       â”‚       â””â”€â”€ *.yaml
â”‚       â”œâ”€â”€ macros/              # Reusable SQL snippets
â”‚       â”œâ”€â”€ schemas/             # Schema definitions
â”‚       â””â”€â”€ notebooks/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ profiles/
â”‚   â”‚   â”œâ”€â”€ tiny.yaml            # 4GB VM
â”‚   â”‚   â”œâ”€â”€ small.yaml           # 20GB VM (default)
â”‚   â”‚   â”œâ”€â”€ medium.yaml          # 64GB VM
â”‚   â”‚   â””â”€â”€ large.yaml           # 128GB+ VM
â”‚   â””â”€â”€ nessie/
â”‚       â””â”€â”€ application.yaml     # Nessie config
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”‚
â”œâ”€â”€ docker-compose.yml           # Full stack
â”œâ”€â”€ docker-compose.minimal.yml   # DuckDB + MinIO only (no Nessie)
â”œâ”€â”€ .github/workflows/ci.yml
â”œâ”€â”€ .pre-commit-config.yaml
â””â”€â”€ pyproject.toml
```

---

## ðŸ”§ Core Components Detail

### 1. Pipeline SDK (dbt-like)

#### SQL Pipeline Example
```sql
-- workspaces/acme/pipelines/silver/sales.sql

-- Pipeline metadata (parsed by Ratatouille)
-- @name: silver_sales
-- @materialized: incremental
-- @unique_key: txn_id
-- @partition_by: _date
-- @owner: data-team

SELECT
    txn_id,
    store_id,
    product_id,
    quantity,
    unit_price,
    quantity * unit_price AS total_amount,
    payment_method,
    transaction_time,
    CAST(transaction_time AS DATE) AS _date,
    NOW() AS _processed_at
FROM {{ ref('bronze.raw_sales') }}
WHERE quantity > 0
  AND unit_price > 0
{% if is_incremental() %}
  AND transaction_time > '{{ watermark("transaction_time") }}'
{% endif %}
```

#### YAML Config Example
```yaml
# workspaces/acme/pipelines/silver/sales.yaml

description: |
  Cleaned and validated sales transactions.
  - Filters invalid records (qty <= 0, price <= 0)
  - Adds calculated total_amount
  - Partitioned by date for efficient incremental processing

owner: data-team@acme.com

# Schema definition & tests
columns:
  - name: txn_id
    type: string
    description: Unique transaction identifier
    tests:
      - not_null
      - unique

  - name: store_id
    type: string
    tests: [not_null]

  - name: quantity
    type: int
    tests:
      - not_null
      - positive

  - name: total_amount
    type: decimal(12,2)
    tests:
      - not_null
      - positive

  - name: _date
    type: date
    description: Partition column
    tests: [not_null]

# Freshness SLA
freshness:
  warn_after: { hours: 6 }
  error_after: { hours: 24 }

# Custom tests
tests:
  - name: total_equals_qty_times_price
    sql: |
      SELECT COUNT(*) FROM {{ this }}
      WHERE ABS(total_amount - quantity * unit_price) > 0.01
    expect: 0
```

#### Python Pipeline Example (for ingestion)
```python
# workspaces/acme/pipelines/bronze/ingest_sales.py

from ratatouille import pipeline, ingest
from ratatouille.parsers import excel_parser
import re

@pipeline(
    name="bronze_raw_sales",
    layer="bronze",
    schedule="0 * * * *",  # Hourly
)
def ingest_sales():
    """Ingest sales files from landing zone."""

    files = rat.ls("landing/sales/*.xlsx")

    for file_path in files:
        # Skip already processed
        if rat.is_ingested(file_path):
            continue

        # Extract metadata from filename
        # e.g., "sales_STORE001_2024-01-15.xlsx"
        match = re.match(r"sales_(\w+)_(\d{4}-\d{2}-\d{2})\.xlsx", file_path)
        if not match:
            rat.log.warning(f"Skipping {file_path}: unexpected filename format")
            continue

        store_id, date = match.groups()

        # Parse Excel with custom logic
        df = excel_parser(
            rat.read_file(file_path),
            skip_rows=3,  # Header junk
            columns_map={
                "Transaction ID": "txn_id",
                "Qty": "quantity",
                "Price": "unit_price",
            }
        )

        # Add metadata
        df["store_id"] = store_id
        df["_source_file"] = file_path
        df["_ingested_at"] = rat.now()

        # Append to bronze table
        rat.append("bronze.raw_sales", df)
        rat.mark_ingested(file_path)

        rat.log.info(f"âœ… Ingested {len(df)} rows from {file_path}")
```

---

### 2. Workspace Configuration

```yaml
# workspaces/acme/workspace.yaml

name: acme
version: "1.0"
description: "ACME Corp data workspace"

# Isolation settings
isolation:
  nessie_branch: "workspace/acme"  # Dedicated Nessie branch
  s3_prefix: "acme"                 # s3://acme/bronze/*, s3://acme/silver/*

# Resource limits for this workspace
resources:
  profile: small  # Use small.yaml profile
  overrides:
    max_memory_mb: 4096
    max_parallel_pipelines: 2

# Medallion layer config
layers:
  bronze:
    retention_days: 90
    partition_by: [_ingested_date]
  silver:
    retention_days: 365
    partition_by: [_date]
  gold:
    retention_days: null  # Forever
    partition_by: []

# Data products this workspace publishes
products:
  - name: sales_kpis
    source: gold.daily_sales_kpis
    access:
      - workspace: "*"
        level: read  # Public within org
    sla:
      freshness_hours: 24

# Data products this workspace consumes
subscriptions:
  - product: inventory/stock_levels
    alias: ext_inventory  # Access as {{ ref('ext_inventory') }}
```

---

### 3. Data Products

```python
# Publishing a data product
rat.publish(
    source="gold.daily_sales_kpis",
    product="sales_kpis",
    version="2.1.0",
    description="Daily sales KPIs by store and product category",
    schema="schemas/sales_kpis.yaml",
    access=[
        {"workspace": "analytics", "level": "read"},
        {"workspace": "reporting", "level": "read"},
    ]
)

# Consuming in another workspace
# In SQL:
SELECT * FROM {{ ref('products.sales_kpis') }}

# In Python:
df = rat.consume("sales_kpis")
df = rat.consume("sales_kpis", version="2.x")  # Semver
```

**Product Registry Schema:**
```sql
-- Stored in shared SQLite or PostgreSQL

CREATE TABLE products (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    version TEXT NOT NULL,
    owner_workspace TEXT NOT NULL,
    description TEXT,
    schema_json TEXT,
    storage_path TEXT,  -- s3://products/sales_kpis/v2.1.0/
    iceberg_snapshot_id BIGINT,
    created_at TIMESTAMP,
    created_by TEXT,
    UNIQUE(name, version)
);

CREATE TABLE product_access (
    id TEXT PRIMARY KEY,
    product_name TEXT NOT NULL,
    workspace TEXT NOT NULL,  -- '*' for all
    access_level TEXT NOT NULL,  -- 'read', 'write', 'none'
    granted_at TIMESTAMP,
    granted_by TEXT
);

CREATE TABLE product_subscriptions (
    id TEXT PRIMARY KEY,
    product_name TEXT NOT NULL,
    consumer_workspace TEXT NOT NULL,
    alias TEXT,  -- Local name in consumer workspace
    version_constraint TEXT,  -- '2.x', '^2.0.0', 'latest'
    subscribed_at TIMESTAMP
);
```

---

### 4. Nessie Integration

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NESSIE CATALOG STRUCTURE                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Branches:                                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚   main                        â† Production (protected)          â”‚
â”‚   â”œâ”€â”€ workspace/acme          â† Workspace A isolation           â”‚
â”‚   â”‚   â”œâ”€â”€ dev/feature-x       â† Dev branch for feature          â”‚
â”‚   â”‚   â””â”€â”€ dev/bugfix-y        â† Another dev branch              â”‚
â”‚   â”œâ”€â”€ workspace/analytics     â† Workspace B isolation           â”‚
â”‚   â””â”€â”€ products                â† Published data products          â”‚
â”‚                                                                  â”‚
â”‚   Tables (in each branch):                                      â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚   bronze.raw_sales                                               â”‚
â”‚   bronze.raw_inventory                                           â”‚
â”‚   silver.sales                                                   â”‚
â”‚   silver.inventory                                               â”‚
â”‚   gold.daily_sales_kpis                                          â”‚
â”‚   gold.inventory_snapshot                                        â”‚
â”‚                                                                  â”‚
â”‚   Operations:                                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚   â€¢ Commit: atomic multi-table updates                          â”‚
â”‚   â€¢ Merge: promote dev â†’ workspace â†’ main                       â”‚
â”‚   â€¢ Cherry-pick: selective changes                              â”‚
â”‚   â€¢ Time-travel: query any commit                               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Nessie Docker Setup:**
```yaml
# docker-compose.yml

services:
  nessie:
    image: ghcr.io/projectnessie/nessie:latest
    container_name: ratatouille-nessie
    environment:
      NESSIE_VERSION_STORE_TYPE: ROCKSDB
      QUARKUS_HTTP_PORT: 19120
    ports:
      - "19120:19120"
    volumes:
      - nessie_data:/nessie/data
    deploy:
      resources:
        limits:
          memory: 512M  # Lightweight!
```

---

### 5. DuckDB + Iceberg

```python
# src/ratatouille/engine/duckdb.py

import duckdb
from pathlib import Path

class DuckDBEngine:
    """DuckDB with Iceberg extension for Ratatouille."""

    def __init__(self, workspace: "Workspace"):
        self.workspace = workspace
        self.conn = duckdb.connect()
        self._setup_extensions()
        self._setup_iceberg()

    def _setup_extensions(self):
        """Load required extensions."""
        self.conn.execute("INSTALL iceberg; LOAD iceberg;")
        self.conn.execute("INSTALL httpfs; LOAD httpfs;")

        # Configure S3/MinIO
        self.conn.execute(f"""
            SET s3_endpoint = '{self.workspace.s3_endpoint}';
            SET s3_access_key_id = '{self.workspace.s3_key}';
            SET s3_secret_access_key = '{self.workspace.s3_secret}';
            SET s3_url_style = 'path';
        """)

    def _setup_iceberg(self):
        """Configure Iceberg catalog (Nessie)."""
        self.conn.execute(f"""
            ATTACH '{self.workspace.nessie_uri}' AS iceberg (
                TYPE ICEBERG,
                CATALOG_TYPE NESSIE,
                NESSIE_REF '{self.workspace.nessie_branch}'
            );
        """)

    def query(self, sql: str) -> "DataFrame":
        """Execute query with memory limits."""
        # Set memory limit based on profile
        max_mem = self.workspace.resources.max_memory_mb
        self.conn.execute(f"SET memory_limit = '{max_mem}MB';")

        return self.conn.execute(sql).df()

    def execute_pipeline(self, pipeline: "Pipeline"):
        """Execute a compiled pipeline."""
        sql = pipeline.compile(self.workspace)

        if pipeline.is_incremental:
            # Get watermark from last run
            watermark = self._get_watermark(pipeline.name)
            sql = sql.replace("{{ watermark }}", watermark)

        # Execute and write to Iceberg
        result = self.query(sql)
        self._write_iceberg(pipeline.target, result, pipeline.unique_key)

        # Update watermark
        if pipeline.is_incremental:
            self._update_watermark(pipeline.name, result)

        return {"rows": len(result)}
```

---

### 6. Resource Profiles

```yaml
# config/profiles/small.yaml (20GB VM - YOUR TARGET)

description: "Optimized for 20GB RAM VMs"

resources:
  # Memory limits
  max_memory_mb: 4096          # Max per pipeline
  duckdb_memory_mb: 8192       # DuckDB total

  # Processing
  chunk_size_rows: 50000       # Batch size for large tables
  max_parallel_pipelines: 2    # Concurrent pipelines

  # DuckDB settings
  duckdb:
    threads: 2
    memory_limit: "8GB"
    temp_directory: "/tmp/duckdb"

  # Nessie
  nessie:
    memory_mb: 512

  # MinIO
  minio:
    memory_mb: 1024


# config/profiles/tiny.yaml (4GB VM / Raspberry Pi)

description: "Minimal footprint for tiny VMs"

resources:
  max_memory_mb: 1024
  duckdb_memory_mb: 2048
  chunk_size_rows: 10000
  max_parallel_pipelines: 1

  duckdb:
    threads: 1
    memory_limit: "2GB"


# config/profiles/large.yaml (128GB+ VM)

description: "Full performance for large VMs"

resources:
  max_memory_mb: 32768
  duckdb_memory_mb: 65536
  chunk_size_rows: 500000
  max_parallel_pipelines: 8

  duckdb:
    threads: 8
    memory_limit: "64GB"
```

---

## ðŸ³ Docker Compose

```yaml
# docker-compose.yml

version: "3.8"

services:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Storage: MinIO (S3-compatible)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  minio:
    image: minio/minio:latest
    container_name: ratatouille-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-ratatouille}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-ratatouille123}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    deploy:
      resources:
        limits:
          memory: ${MINIO_MEMORY:-1G}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 5s
      retries: 3

  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        mc alias set minio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD}
        mc mb --ignore-existing minio/warehouse
        mc mb --ignore-existing minio/products
        echo "âœ… Buckets ready!"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Catalog: Nessie (Git-like Iceberg catalog)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  nessie:
    image: ghcr.io/projectnessie/nessie:latest
    container_name: ratatouille-nessie
    environment:
      NESSIE_VERSION_STORE_TYPE: ROCKSDB
      QUARKUS_HTTP_PORT: 19120
    ports:
      - "19120:19120"
    volumes:
      - nessie_data:/nessie/data
    deploy:
      resources:
        limits:
          memory: ${NESSIE_MEMORY:-512M}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/api/v2/config"]
      interval: 5s
      timeout: 5s
      retries: 5

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Orchestration: Dagster
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  dagster:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ratatouille-dagster
    environment:
      # S3
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: ${MINIO_ROOT_USER:-ratatouille}
      S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-ratatouille123}
      # Nessie
      NESSIE_URI: http://nessie:19120/api/v2
      # Resources
      RAT_PROFILE: ${RAT_PROFILE:-small}
    ports:
      - "3030:3000"
    volumes:
      - ./src:/app/src
      - ./workspaces:/app/workspaces
      - dagster_storage:/app/storage
    depends_on:
      minio:
        condition: service_healthy
      nessie:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: ${DAGSTER_MEMORY:-2G}
    command: dagster dev -h 0.0.0.0 -p 3000

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Development: JupyterLab
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ratatouille-jupyter
    environment:
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: ${MINIO_ROOT_USER:-ratatouille}
      S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-ratatouille123}
      NESSIE_URI: http://nessie:19120/api/v2
      JUPYTER_TOKEN: ${JUPYTER_TOKEN:-ratatouille}
      RAT_PROFILE: ${RAT_PROFILE:-small}
    ports:
      - "8889:8888"
    volumes:
      - ./src:/app/src
      - ./workspaces:/app/workspaces
    depends_on:
      minio:
        condition: service_healthy
      nessie:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: ${JUPYTER_MEMORY:-2G}
    command: >
      jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
      --ServerApp.token=${JUPYTER_TOKEN:-ratatouille}
      --notebook-dir=/app/workspaces

networks:
  default:
    driver: bridge

volumes:
  minio_data:
  nessie_data:
  dagster_storage:
```

---

## ðŸ“‹ Migration Plan

### Phase 1: Core Infrastructure âœ… COMPLETE
- [x] Set up Nessie service (docker-compose.yml)
- [x] Implement DuckDB + Iceberg integration (engine/duckdb.py)
- [x] Create workspace isolation layer (workspace/manager.py, workspace/config.py)
- [x] Build resource profiles system (resources/config.py, resources/profiles.py)

### Phase 2: Pipeline SDK âœ… COMPLETE
- [x] SQL pipeline parser with {{ ref() }}, {% if %} (pipeline/parser.py)
- [x] YAML config loader (pipeline/config.py)
- [x] Incremental processing with watermarks (pipeline/incremental.py)
- [x] Python pipeline decorators (pipeline/decorators.py)
- [x] Pipeline loader and DAG builder (pipeline/loader.py)
- [x] Pipeline executor (pipeline/executor.py)

### Phase 3: Data Products âœ… COMPLETE
- [x] Product registry with SQLite storage (products/registry.py)
- [x] Publish APIs with schema extraction (products/publish.py)
- [x] Consume APIs with version resolution (products/consume.py)
- [x] Permission system with patterns (products/permissions.py)
- [x] Semver version management with constraints (^1.0.0, ~1.2.0, etc.)

### Phase 4: Migration & Polish âœ… COMPLETE
- [x] Migrated example pipelines to new dbt-like format
- [x] CI/CD setup (GitHub Actions: lint, typecheck, unit tests, integration tests, docker)
- [x] Documentation (guides: pipelines.md, data-products.md, workspaces.md)
- [x] Unit tests for parser, products, workspace
- [x] Integration tests for DuckDB engine
- [x] Updated pyproject.toml with proper packaging

---

## âœ… Success Criteria

| Metric | Current | Target |
|--------|---------|--------|
| Memory usage (1M row merge) | OOM | < 500MB |
| Memory usage (10B row query) | N/A | < 4GB (streaming) |
| Pipeline code duplication | ~60% | < 5% |
| Time to create workspace | Manual | < 2 min |
| Schema validation | 0% | 100% |
| Lineage tracking | None | Full DAG |
| Data product creation | N/A | < 5 min |

---

## ðŸ”— References

- [Project Nessie](https://projectnessie.org/)
- [DuckDB Iceberg Extension](https://duckdb.org/docs/extensions/iceberg)
- [Apache Iceberg](https://iceberg.apache.org/)
- [dbt Core Concepts](https://docs.getdbt.com/docs/build/projects)

---

*"Anyone can data!" ðŸ€*
